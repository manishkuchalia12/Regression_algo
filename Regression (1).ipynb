{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e959c3eb-ed7c-40f3-8f60-2aff94cb68e3",
   "metadata": {},
   "source": [
    "Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it \n",
    "represent?\n",
    "Ans:-R-squared (R²) is a statistical measure that represents the proportion of the variance in the dependent variable (the variable being predicted) that is explained by the independent variable(s) in a regression model. In the context of linear regression, R-squared indicates the goodness of fit of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2a4005-eeb4-4ac1-ad52-b9eca75279af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)  # Independent variable\n",
    "y = np.array([2, 4, 5, 4, 5])  # Dependent variable\n",
    "\n",
    "# Create a linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model to the data\n",
    "model.fit(X, y)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# Calculate R-squared\n",
    "r_squared = r2_score(y, y_pred)\n",
    "\n",
    "print(\"R-squared:\", r_squared)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7ebb0d-3cc4-4f32-b75d-572377609eb4",
   "metadata": {},
   "source": [
    "Q2. Define adjusted R-squared and explain how it differs from the regular R-squared. \n",
    "Ans:-Adjusted R-squared is a modification of the traditional R-squared (coefficient of determination) that accounts for the number of predictors (independent variables) in a regression model. While R-squared tells you the proportion of the variance in the dependent variable that is explained by the model, adjusted R-squared adjusts this value based on the number of predictors and provides a more accurate measure of a model's goodness of fit.\n",
    "Key points about adjusted R-squared:\r\n",
    "\r\n",
    "Penalty for Adding Variables: If adding a new variable to the model does not improve the fit significantly, the adjusted R-squared will decrease. This helps prevent overfitting by discouraging the inclusion of unnecessary variable.\r\n",
    "\r\n",
    "Dependence on Sample Size and Variables: The adjusted R-squared depends on both the sample s\n",
    "�\r\n",
    "n) and the number of pred\r\n",
    "�\r\n",
    "k), providing a more nuanced evaluation of model perfrmance.\r\n",
    "\r\n",
    "Range: Like R-squared, adjusted R-squared values range from 0 to 1. A higher adjusted R-squared indicates a better fit, but it considers the impact of model complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe03c7a1-f067-4862-87ce-021faf15f886",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)  # Independent variable\n",
    "y = np.array([2, 4, 5, 4, 5])  # Dependent variable\n",
    "\n",
    "# Add a constant term to the independent variable matrix\n",
    "X_with_constant = sm.add_constant(X)\n",
    "\n",
    "# Create a linear regression model using statsmodels\n",
    "model = sm.OLS(y, X_with_constant).fit()\n",
    "\n",
    "# Get the R-squared and the number of predictors\n",
    "r_squared = model.rsquared\n",
    "num_predictors = len(model.params) - 1  # Exclude the intercept term\n",
    "\n",
    "# Calculate the adjusted R-squared\n",
    "n = len(y)\n",
    "adjusted_r_squared = 1 - ((1 - r_squared) * (n - 1) / (n - num_predictors - 1))\n",
    "\n",
    "print(\"Adjusted R-squared:\", adjusted_r_squared)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e9ade9-5efb-45d3-b3fb-b8656753125b",
   "metadata": {},
   "source": [
    "Q3. When is it more appropriate to use adjusted R-squared?\n",
    "Ans:-Adjusted R-squared is more appropriate to use when comparing and evaluating models with different numbers of predictors (independent variables). It helps address the potential issue of overfitting and provides a more nuanced assessment of a model's goodness of fit by penalizing the inclusion of unnecessary variables.\n",
    "\r\n",
    "Here are situations in which adjusted R-squared is particularly usefu:\r\n",
    "\r\n",
    "Comparing Models: When you have multiple regression models with different numbers of predictors, comparing their adjusted R-squared values can help you determine which model provides the best balance between goodness of fit and model complexity. Adjusted R-squared accounts for the trade-off between adding more predictors and improving the it.\r\n",
    "\r\n",
    "Variable Selection: In situations where you are considering adding or removing predictors from your model, adjusted R-squared can guide the variable selection process. It discourages the inclusion of variables that do not significantly contribute to improving th fit.\r\n",
    "\r\n",
    "Preventing Overfitting: Overfitting occurs when a model captures noise in the training data rather than the underlying pattern. Adjusted R-squared helps mitigate overfitting by penalizing the model for including variables that don't contribute meaningfully to the explanation of the dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93dbce2-23f8-4a20-ab81-14640953d193",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)  # Independent variable\n",
    "y = np.array([2, 4, 5, 4, 5])  # Dependent variable\n",
    "\n",
    "# Add a constant term to the independent variable matrix\n",
    "X_with_constant = sm.add_constant(X)\n",
    "\n",
    "# Create two linear regression models using statsmodels with different numbers of predictors\n",
    "model1 = sm.OLS(y, X_with_constant).fit()  # Model with one predictor\n",
    "model2 = sm.OLS(y, sm.add_constant(np.column_stack((X, X**2)))).fit()  # Model with two predictors (quadratic term)\n",
    "\n",
    "# Calculate R-squared and adjusted R-squared for each model\n",
    "r_squared1 = model1.rsquared\n",
    "adjusted_r_squared1 = model1.rsquared_adj\n",
    "\n",
    "r_squared2 = model2.rsquared\n",
    "adjusted_r_squared2 = model2.rsquared_adj\n",
    "\n",
    "# Display the results\n",
    "print(\"Model 1 - R-squared:\", r_squared1, \"Adjusted R-squared:\", adjusted_r_squared1)\n",
    "print(\"Model 2 - R-squared:\", r_squared2, \"Adjusted R-squared:\", adjusted_r_squared2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e65c9ef-dc10-4270-b28f-bb7464147afa",
   "metadata": {},
   "source": [
    "Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics \n",
    "calculated, and what do they represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab4f736-065d-4815-9162-be6f8823bc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "y_actual = np.array([2, 4, 5, 4, 5])  # Actual values\n",
    "y_predicted = np.array([2.5, 3.8, 4.2, 3.5, 5.1])  # Predicted values\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_actual, y_predicted)\n",
    "\n",
    "# Root Mean Squared Error (RMSE)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# Mean Absolute Error (MAE)\n",
    "mae = mean_absolute_error(y_actual, y_predicted)\n",
    "\n",
    "# Display the results\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39397a26-3d03-4afd-8790-6d2491015a24",
   "metadata": {},
   "source": [
    "Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in \n",
    "regression analysis.\n",
    "Ans:-Mean Squared Error (MSE):\r\n",
    "Advantages\r\n",
    "\r\n",
    "Sensitivity to Errors: Squaring the errors penalizes larger errors more, which might be desirable if large errors are considered more critical.\r\n",
    "Mathematical Simplicity: The squared term makes the calculation of gradients and mathematical operations simpler.\r\n",
    "Disadvantges:\r\n",
    "\r\n",
    "Sensitivity to Outliers: MSE gives higher weight to outliers due to the squaring, which may not be suitable if your data contains extreme values.\r\n",
    "Units: The units of MSE are the square of the units of the dependent variable, which might make interpretation difficult.\r\n",
    "Root Mean Squared Error (RMSE):\r\n",
    "Avantages:\r\n",
    "\r\n",
    "Interpretability: RMSE is on the same scale as the dependent variable, making it easier to interpret compared to MSE.\r\n",
    "Sensitivity to Errors: Like MSE, it penalizes larger errors more, providing a balance between sensitivity and interpretability.\r\n",
    "isadvantages:\r\n",
    "\r\n",
    "Sensitivity to Outliers: Similar to MSE, RMSE can be sensitive to outliers due to the squared term.\r\n",
    "Units: Like MSE, RMSE inherits the square of the units of the dependent variable.\r\n",
    "Mean Absolute Error MAE):\r\n",
    "Advantages:\r\n",
    "\r\n",
    "Robustness to Outliers: MAE is less sensitive to outliers since it does not square the errors.\r\n",
    "Interpretability: MAE is in the same units as the dependent variable, making it easily interprtable.\r\n",
    "Disadvantages:\r\n",
    "\r\n",
    "Equal Weight to All Errors: MAE treats all errors equally, which may not be appropriate if certain errors are more critical than others.\r\n",
    "Mathematical Complexity: The absolute value makes mathematical operations involving gradients less straightforward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce4045b-842c-4362-8e7f-9a36c28ed1c8",
   "metadata": {},
   "source": [
    "Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is \n",
    "it more appropriate to use?\n",
    "Ans:-Differences and When to Use Lasso:\r\n",
    "Sparse Solutions: Lasso tends to produce sparse coefficient vectors by driving some coefficients to exactly zero. This makes it useful for feature selection, where irrelevant features are automatically excluded from the model\r\n",
    "\r\n",
    "Feature Selection: If you suspect that only a small number of features are truly relevant in your regression model, Lasso is a good choie.\r\n",
    "\r\n",
    "Less Tolerance for Outliers: Lasso is more sensitive to outliers than Ridge due to the use of the absolute values of coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fd63dd-07ad-42d2-85cd-6b36fe242ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_regression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a synthetic dataset\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create and fit a Lasso regression model\n",
    "alpha = 0.1  # Regularization parameter\n",
    "lasso_model = Lasso(alpha=alpha)\n",
    "lasso_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Print the coefficients\n",
    "print(\"Lasso Coefficients:\", lasso_model.coef_)\n",
    "\n",
    "# Plot the regression line\n",
    "plt.scatter(X_test, y_test, label='Test Data')\n",
    "plt.plot(X_test, lasso_model.predict(X_test_scaled), color='red', label='Lasso Regression')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Target')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6dfe2f-eb1f-42f3-a879-04fd65112479",
   "metadata": {},
   "source": [
    "Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an \n",
    "example to illustrate.\n",
    "Ans:-There are two common types of regularization for linear models: L1 regularization (Lasso) and L2 regularization (Ridge).\n",
    "\r\n",
    "L1 Regularization (Lasso:\r\n",
    "\r\n",
    "The penalty term is the sum of the absolute values of the coefficients.\r\n",
    "Encourages sparse solutions by driving some coefficients to exactly zero.\r\n",
    "Useful for feature selection.\r\n",
    "L2 Regularization (idge):\r\n",
    "\r\n",
    "The penalty term is the sum of the squared values of the coefficients.\r\n",
    "Encourages smaller, but non-zero, coefficients.\r\n",
    "Reduces the impact of individual features without necessarily excluding them entirely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7ce4e7-9f36-486f-8a65-08843b41988d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Create synthetic data with a quadratic relationship\n",
    "np.random.seed(42)\n",
    "X = np.sort(5 * np.random.rand(80, 1), axis=0)\n",
    "y = np.sin(X).ravel() + np.random.normal(0, 0.1, X.shape[0])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Polynomial regression without regularization (degree=15)\n",
    "degree = 15\n",
    "model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Polynomial regression with Lasso regularization (L1)\n",
    "alpha_lasso = 0.01\n",
    "lasso_model = make_pipeline(PolynomialFeatures(degree), Lasso(alpha=alpha_lasso))\n",
    "lasso_model.fit(X_train, y_train)\n",
    "\n",
    "# Polynomial regression with Ridge regularization (L2)\n",
    "alpha_ridge = 0.01\n",
    "ridge_model = make_pipeline(PolynomialFeatures(degree), Ridge(alpha=alpha_ridge))\n",
    "ridge_model.fit(X_train, y_train)\n",
    "\n",
    "# Plot the results\n",
    "X_plot = np.arange(0, 5, 0.01)[:, np.newaxis]\n",
    "plt.scatter(X, y, s=20, edgecolor=\"black\", c=\"darkorange\", label=\"data\")\n",
    "plt.plot(X_plot, model.predict(X_plot), color=\"cornflowerblue\", label=\"Linear Regression\")\n",
    "plt.plot(X_plot, lasso_model.predict(X_plot), color=\"red\", label=\"Lasso Regression\")\n",
    "plt.plot(X_plot, ridge_model.predict(X_plot), color=\"green\", label=\"Ridge Regression\")\n",
    "plt.xlabel(\"data\")\n",
    "plt.ylabel(\"target\")\n",
    "plt.title(\"Polynomial Regression with Regularization\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b53fa45-9c02-4880-9a4d-4cb364736285",
   "metadata": {},
   "source": [
    "Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best \n",
    "choice for regression analysis.\n",
    "Ans:-While regularized linear models are powerful tools for preventing overfitting and addressing multicollinearity in regression analysis, they have some limitations that may make them less suitable in certain situations. Here are some of the key limitations:\n",
    "\r\n",
    "Sensitivity to Scalin:\r\n",
    "\r\n",
    "Regularized linear models are sensitive to the scale of the features. If features are not scaled properly, the regularization term may impact some features more than others, leading to biased coefficient estimates. It's essential to standardize or normalize features before applying regularization.\r\n",
    "Inability to Handle Categorical Variables Diretly:\r\n",
    "\r\n",
    "Regularization techniques are designed for numerical features, and they may not handle categorical variables directly. One-hot encoding or other preprocessing steps are often required to use regularized models with categorical variables.\r\n",
    "Loss of Interpretbility:\r\n",
    "\r\n",
    "The penalty terms in regularization, especially in L1 regularization (Lasso), may drive some coefficients to exactly zero. While this can be beneficial for feature selection, it also leads to a loss of interpretability, as certain features are effectively excluded from the model.\r\n",
    "Model SelectionChallenge:\r\n",
    "\r\n",
    "Choosing the appropriate regularization strength (alpha) can be challenging. If the regularization strength is too high, the model may underfit the data, while if it's too low, the model may overfit. Cross-validation is often used to find the optimal value for alpha, but this adds complexity to the model selection process.\r\n",
    "Limited Effectiveness with Large Fature Spaces:\r\n",
    "\r\n",
    "Regularization may be less effective in high-dimensional feature spaces, especially when the number of features is much larger than the number of observations. In such cases, other techniques like feature engineering or dimensionality reduction might be more appropriate.\r\n",
    "Non-Smooth Obective Function:\r\n",
    "\r\n",
    "The regularization terms lead to non-smooth objective functions, making optimization more challenging. This can affect the convergence speed of optimization algorithms, particularly in complex models or large datasets.\r\n",
    "Potential oss of Information:\r\n",
    "\r\n",
    "In some cases, aggressive regularization may lead to a significant reduction in model flexibility, potentially causing a loss of important information present in the data.\r\n",
    "Asumption of Linearity:\r\n",
    "\r\n",
    "Regularized linear models assume a linear relationship between the features and the target variable. If the true relationship is highly non-linear, other non-linear models might be more apprpriate.\r\n",
    "Outliers Impact:\r\n",
    "\r\n",
    "L1 regularization (Lasso) is sensitive to outliers, as the absolute values of the coefficients can be influenced by extreme data points. Outliers can disproportionately impact the regularization term and the resulting model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e9e888-ec44-4c79-a983-2fd8c787f1fb",
   "metadata": {},
   "source": [
    "Q9. You are comparing the performance of two regression models using different evaluation metrics. \n",
    "Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better \n",
    "performer, and why? Are there any limitations to your choice of metric?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
